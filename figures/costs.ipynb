{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc9b7681",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MLP Architectures\n",
    "mlp1 = {\"net\": \"mlp\", \"optimizer\": \"sgd\", \"layers\": [{\"d_in\": 784, \"d_out\": 100}, {\"d_in\": 100, \"d_out\": 50}, {\"d_in\": 50, \"d_out\": 10}]}\n",
    "mlp2 = {\"net\": \"mlp\", \"optimizer\": \"sgd\", \"layers\": [{\"d_in\": 784, \"d_out\": 200}, {\"d_in\": 200, \"d_out\": 100}, {\"d_in\": 100, \"d_out\": 50}, {\"d_in\": 50, \"d_out\": 10}]}\n",
    "mlp3 = {\"net\": \"mlp\", \"optimizer\": \"adam\", \"layers\": [{\"d_in\": 784, \"d_out\": 1024}, {\"d_in\": 1024, \"d_out\": 1024}, {\"d_in\": 1024, \"d_out\": 1024}, {\"d_in\": 1024, \"d_out\": 10}]}\n",
    "mlp4 = {\"net\": \"mlp\", \"optimizer\": \"adam\", \"layers\": [{\"d_in\": 1024, \"d_out\": 3000}, {\"d_in\": 3000, \"d_out\": 3000}, {\"d_in\": 3000, \"d_out\": 3000}, {\"d_in\": 3000, \"d_out\": 10}]}\n",
    "\n",
    "# Conv Architectures\n",
    "conv1 = {\"net\": \"conv\", \"optimizer\": \"adam\", \"layers\": [{\"d_in\": 32, \"d_out\": 32, \"c_in\": 3, \"c_out\": 128, \"k\": 3}, {\"d_in\": 32, \"d_out\": 32, \"c_in\": 128, \"c_out\": 256, \"k\": 3}, {\"d_in\": 16, \"d_out\": 16, \"c_in\": 256, \"c_out\": 256, \"k\": 3}, {\"d_in\": 16, \"d_out\": 16, \"c_in\": 256, \"c_out\": 512, \"k\": 3}, {\"d_in\": 8, \"d_out\": 8, \"c_in\": 512, \"c_out\": 512, \"k\": 3}, {\"d_in\": 4, \"d_out\": 4, \"c_in\": 512, \"c_out\": 512, \"k\": 3}, {\"d_in\": 2048, \"d_out\": 1024}, {\"d_in\": 1024, \"d_out\": 10}]}\n",
    "conv2 = {\"net\": \"conv\", \"optimizer\": \"adam\", \"layers\": [{\"d_in\": 32, \"d_out\": 32, \"c_in\": 3, \"c_out\": 128, \"k\": 3}, {\"d_in\": 32, \"d_out\": 32, \"c_in\": 128, \"c_out\": 128, \"k\": 3}, {\"d_in\": 32, \"d_out\": 32, \"c_in\": 128, \"c_out\": 128, \"k\": 3}, {\"d_in\": 32, \"d_out\": 32, \"c_in\": 128, \"c_out\": 256, \"k\": 3}, {\"d_in\": 16, \"d_out\": 16, \"c_in\": 256, \"c_out\": 256, \"k\": 3}, {\"d_in\": 16, \"d_out\": 16, \"c_in\": 256, \"c_out\": 512, \"k\": 3}, {\"d_in\": 8, \"d_out\": 8, \"c_in\": 512, \"c_out\": 512, \"k\": 3}, {\"d_in\": 8, \"d_out\": 8, \"c_in\": 512, \"c_out\": 512, \"k\": 3}, {\"d_in\": 4, \"d_out\": 4, \"c_in\": 512, \"c_out\": 512, \"k\": 3}, {\"d_in\": 2048, \"d_out\": 1024}, {\"d_in\": 1024, \"d_out\": 10}]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5f0c59d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "nitro = False          # False = SGD, True = Our solution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d72958d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP0 - FP Backprop\n",
      "Energy: 0.0747338356 mJ\n",
      "Memory: 0.890864 MB\n",
      "\n",
      "MLP1 - FP Backprop\n",
      "Energy: 0.1624343956 mJ\n",
      "Memory: 1.6492639999999998 MB\n",
      "\n",
      "MLP2 - FP Backprop\n",
      "Energy: 2.6746635648 mJ\n",
      "Memory: 40.106496 MB\n",
      "\n",
      "MLP3 - FP Backprop\n",
      "Energy: 19.395862041599997 mJ\n",
      "Memory: 291.792704 MB\n",
      "\n",
      "CONV4 - FP Backprop\n",
      "Energy: 838.7449070976 mJ\n",
      "Memory: 278.694912 MB\n",
      "\n",
      "CONV5 - FP Backprop\n",
      "Energy: 1238.8826883456002 mJ\n",
      "Memory: 386.04288 MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# ===== Config =====\n",
    "bs = 64                     # batch size\n",
    "LOCAL_FC_IN_CONV = 4096     # conv-layer local head input dim\n",
    "LOCAL_FC_OUT = 10           # local head output classes\n",
    "   \n",
    "for i, net in enumerate([mlp1, mlp2, mlp3, mlp4, conv1, conv2]):\n",
    "    if nitro:\n",
    "        print(net[\"net\"].upper() + f\"{i} - Integer NITRO\")\n",
    "    else:\n",
    "        print(net[\"net\"].upper() + f\"{i} - FP Backprop\")\n",
    "\n",
    "    # SGD update costs (per param); no momentum\n",
    "    if nitro or net[\"optimizer\"] == \"sgd\":\n",
    "        PER_PARAM_MUL = 2\n",
    "        PER_PARAM_ADD = 2\n",
    "        OPT_STATE_MULT = 0          # persistent optimizer state elements per param\n",
    "    else:\n",
    "        PER_PARAM_MUL = 9\n",
    "        PER_PARAM_ADD = 4\n",
    "        OPT_STATE_MULT = 2          # m, v per param\n",
    "\n",
    "    # ---- Ops ----\n",
    "    batch_mul, batch_add = 0, 0   # per batch (fwd + bwd over bs)\n",
    "    upd_mul, upd_add = 0, 0       # per-step (optimizer update once)\n",
    "\n",
    "    # ---- Memory (elements, not bytes) ----\n",
    "    mem_w = 0                     # parameters (Σ per layer incl. local heads)\n",
    "    mem_h = 0                     # saved activations (Σ per base layer × bs)\n",
    "    mem_d_peak = 0                # parameter gradients (MAX, streaming)\n",
    "    mem_opt = 0                   # optimizer state (persistent, here 0 with plain SGD)\n",
    "\n",
    "    L = len(net)\n",
    "    for li, layer in enumerate(net[\"layers\"]):\n",
    "        # Decide layer kind: conv if \"c_in\" given; otherwise linear\n",
    "        is_conv = (\"c_in\" in layer)\n",
    "        Cin, Cout, Din, Dout, k = 0, 0, 0, 0, 0\n",
    "        \n",
    "        if is_conv:            \n",
    "            # ---------- CONV layer ----------\n",
    "            Cin  = layer[\"c_in\"]\n",
    "            Cout = layer[\"c_out\"]\n",
    "            Din  = layer[\"d_in\"]          # spatial in (assume square)\n",
    "            Dout = layer[\"d_out\"]         # spatial out (assume square)\n",
    "            k    = layer.get(\"k\", 1)\n",
    "\n",
    "            K   = Cin * k * k\n",
    "            S   = Dout * Dout             # output spatial sites\n",
    "            Sin = Din * Din               # input spatial sites\n",
    "\n",
    "            # ---- per-sample ops (no bias) ----\n",
    "            # forward\n",
    "            mul_fwd = Cout * S * K\n",
    "            add_fwd = Cout * S * (K - 1)\n",
    "\n",
    "            # weight gradients dW\n",
    "            mul_dW = Cout * Cin * k * k * S\n",
    "            # dW adds handled at batch level: (bs*S - 1) per weight\n",
    "\n",
    "            # input gradients dX\n",
    "            mul_dX = Cin * Sin * Cout * k * k\n",
    "            add_dX = Cin * Sin * (Cout * k * k - 1)\n",
    "\n",
    "            # ---- per-batch totals (correct dW adds over batch) ----\n",
    "            batch_mul += bs * (mul_fwd + mul_dW + mul_dX)\n",
    "            dW_add_batch = (Cout * Cin * k * k) * (bs * S - 1)\n",
    "            batch_add += bs * (add_fwd + add_dX) + dW_add_batch\n",
    "\n",
    "            # ---- memory for base layer ----\n",
    "            params = Cout * Cin * k * k\n",
    "            mem_w += params\n",
    "            mem_h += bs * Cout * S\n",
    "            mem_d_peak = max(mem_d_peak, params)\n",
    "\n",
    "            # ---- SGD optimizer update (once per step) ----\n",
    "            upd_mul += params * PER_PARAM_MUL\n",
    "            upd_add += params * PER_PARAM_ADD\n",
    "            mem_opt += params * OPT_STATE_MULT  # stays 0\n",
    "\n",
    "            # ------- local head for HIDDEN conv layers -------\n",
    "            if nitro and (li < L - 2):\n",
    "                lh_in, lh_out = LOCAL_FC_IN_CONV, LOCAL_FC_OUT\n",
    "                lh_params = lh_in * lh_out  # no bias\n",
    "\n",
    "                # ops for linear head (S=1)\n",
    "                lh_mul_fwd = lh_out * lh_in\n",
    "                lh_add_fwd = lh_out * (lh_in - 1)\n",
    "                lh_mul_dW = lh_in * lh_out\n",
    "                lh_add_dW_batch = lh_in * lh_out * (bs - 1)\n",
    "                lh_mul_dX = lh_in * lh_out\n",
    "                lh_add_dX = lh_in * (lh_out - 1)\n",
    "\n",
    "                batch_mul += bs * (lh_mul_fwd + lh_mul_dW + lh_mul_dX)\n",
    "                batch_add += bs * (lh_add_fwd + lh_add_dX) + lh_add_dW_batch\n",
    "\n",
    "                mem_w += lh_params\n",
    "                mem_d_peak = max(mem_d_peak, lh_params)\n",
    "                upd_mul += lh_params * PER_PARAM_MUL\n",
    "                upd_add += lh_params * PER_PARAM_ADD\n",
    "                # no extra mem_h: head uses the base layer activation\n",
    "\n",
    "        else:\n",
    "            # ---------- LINEAR layer ----------\n",
    "            din  = layer[\"d_in\"]          # feature in\n",
    "            dout = layer[\"d_out\"]         # feature out\n",
    "\n",
    "            # per-sample ops (no bias)\n",
    "            mul_fwd = dout * din\n",
    "            add_fwd = dout * (din - 1)\n",
    "\n",
    "            # dW, dX per-sample (dW adds at batch level)\n",
    "            mul_dW = dout * din\n",
    "            mul_dX = din * dout\n",
    "            add_dX = din * (dout - 1)\n",
    "\n",
    "            # per-batch totals (here S=1)\n",
    "            batch_mul += bs * (mul_fwd + mul_dW + mul_dX)\n",
    "            dW_add_batch = (dout * din) * (bs - 1)\n",
    "            batch_add += bs * (add_fwd + add_dX) + dW_add_batch\n",
    "\n",
    "            # memory for base layer\n",
    "            params = dout * din\n",
    "            mem_w += params\n",
    "            mem_h += bs * dout\n",
    "            mem_d_peak = max(mem_d_peak, params)\n",
    "\n",
    "            # SGD update\n",
    "            upd_mul += params * PER_PARAM_MUL\n",
    "            upd_add += params * PER_PARAM_ADD\n",
    "            mem_opt += params * OPT_STATE_MULT\n",
    "\n",
    "            # ------- local head for HIDDEN linear layers -------\n",
    "            if nitro and (li < L - 2):\n",
    "                lh_in, lh_out = dout, LOCAL_FC_OUT\n",
    "                lh_params = lh_in * lh_out  # no bias\n",
    "\n",
    "                lh_mul_fwd = lh_out * lh_in\n",
    "                lh_add_fwd = lh_out * (lh_in - 1)\n",
    "                lh_mul_dW = lh_in * lh_out\n",
    "                lh_add_dW_batch = lh_in * lh_out * (bs - 1)\n",
    "                lh_mul_dX = lh_in * lh_out\n",
    "                lh_add_dX = lh_in * (lh_out - 1)\n",
    "\n",
    "                batch_mul += bs * (lh_mul_fwd + lh_mul_dW + lh_mul_dX)\n",
    "                batch_add += bs * (lh_add_fwd + lh_add_dX) + lh_add_dW_batch\n",
    "\n",
    "                mem_w += lh_params\n",
    "                mem_d_peak = max(mem_d_peak, lh_params)\n",
    "                upd_mul += lh_params * PER_PARAM_MUL\n",
    "                upd_add += lh_params * PER_PARAM_ADD\n",
    "                # no extra mem_h\n",
    "\n",
    "    # ---- Per-step totals ----\n",
    "    step_mul = batch_mul + upd_mul\n",
    "    step_add = batch_add + upd_add\n",
    "\n",
    "    # ---- Costs from https://ieeexplore.ieee.org/abstract/document/6757323 ----\n",
    "    if nitro:\n",
    "        mul_cost = 3.1*10**(-12)\n",
    "        add_cost = 0.1*10**(-12)\n",
    "    else:\n",
    "        mul_cost = 3.7*10**(-12)\n",
    "        add_cost = 0.9*10**(-12)\n",
    "    \n",
    "    # print(\"MUL:\", step_mul)\n",
    "    # print(\"ADD:\", step_add)\n",
    "    \n",
    "    # Convert Joules to mJ, Bytes to MB\n",
    "    print(\"Energy:\", (step_mul*mul_cost+step_add*add_cost) * 1000, \"mJ\")\n",
    "    \n",
    "    input_dim =  net[\"layers\"][0][\"c_in\"] *  net[\"layers\"][0][\"d_in\"]**2 if net[\"net\"] == \"conv\" else net[\"layers\"][0][\"d_in\"]\n",
    "    if nitro:\n",
    "        print(\"Memory:\", (input_dim*bs*8 + mem_w*16 + mem_h*8 + mem_d_peak*32)/8/1000/1000, \"MB\")  # 32 bits per element, /8 to bytes, /1000/1000 to MB\n",
    "    else:\n",
    "        print(\"Memory:\", (input_dim*bs*32 + mem_w*32 + mem_h*32 + mem_d_peak*32 + mem_opt*32)/8/1000/1000, \"MB\")  # 32 bits per element, /8 to bytes, /1000/1000 to MB\n",
    "\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nitro-d (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
